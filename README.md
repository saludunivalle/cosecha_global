# Sistema de Gesti√≥n de Asignaciones Acad√©micas

Sistema para la gesti√≥n y consulta de asignaciones acad√©micas de docentes de la Universidad del Valle.

## üéØ Funcionalidades

### 1. Aplicativo Web
- B√∫squeda de docentes por c√©dula
- Visualizaci√≥n de asignaciones acad√©micas
- Consulta de m√∫ltiples per√≠odos acad√©micos
- Vistas organizadas por per√≠odo o por actividad

### 2. Sistema de Cosecha (Scraper)
- Extracci√≥n autom√°tica de datos desde el portal Univalle
- Procesamiento por per√≠odo individual (arquitectura optimizada)
- Escritura de datos en Google Sheets
- Ejecuci√≥n autom√°tica escalonada v√≠a GitHub Actions
- Procesamiento independiente por per√≠odo para evitar timeouts

## üìÅ Estructura del Proyecto

```
cosecha_global/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ web/              # Aplicativo web (Next.js/React)
‚îÇ   ‚îú‚îÄ‚îÄ harvest/          # Sistema de cosecha de datos
‚îÇ   ‚îú‚îÄ‚îÄ shared/           # C√≥digo compartido
‚îÇ   ‚îî‚îÄ‚îÄ api/              # API Backend
‚îú‚îÄ‚îÄ scraper/              # Scraper de datos acad√©micos
‚îÇ   ‚îú‚îÄ‚îÄ main.py           # Orquestador principal
‚îÇ   ‚îú‚îÄ‚îÄ services/         # Servicios (scraper, sheets, period_manager)
‚îÇ   ‚îú‚îÄ‚îÄ config/           # Configuraci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Utilidades
‚îú‚îÄ‚îÄ .github/workflows/    # GitHub Actions workflows
‚îÇ   ‚îú‚îÄ‚îÄ scraper-2026-1.yml
‚îÇ   ‚îú‚îÄ‚îÄ scraper-2025-2.yml
‚îÇ   ‚îî‚îÄ‚îÄ ... (9 workflows por per√≠odo)
‚îú‚îÄ‚îÄ docs/                 # Documentaci√≥n
‚îú‚îÄ‚îÄ scripts/              # Scripts de utilidad
‚îú‚îÄ‚îÄ public/               # Archivos est√°ticos
‚îî‚îÄ‚îÄ legacy/               # C√≥digo legacy de Apps Script
```

Ver [ESTRUCTURA_PROYECTO.md](./ESTRUCTURA_PROYECTO.md) para m√°s detalles.

## üöÄ Inicio R√°pido

### Prerrequisitos
- Node.js 18+ 
- npm o yarn

### Instalaci√≥n

```bash
# Instalar dependencias
npm install

# Configurar variables de entorno
cp env.example.txt .env
# Editar .env con tus credenciales
# Ver docs/CONFIGURACION_GOOGLE_SHEETS.md para configurar Google Sheets API

# Ejecutar en desarrollo
npm run dev
```

### Desarrollo

```bash
# Aplicativo web
npm run dev

# Sistema de cosecha (si se ejecuta independientemente)
npm run harvest
```

## üìö Documentaci√≥n

- [Documentaci√≥n Completa](./docs/DOCUMENTACION_APLICATIVO.md)
- [Instrucciones del Aplicativo Web](./docs/Intrucciones_AsignacionesAcademicas.md)
- [Configuraci√≥n de Google Sheets API](./docs/CONFIGURACION_GOOGLE_SHEETS.md)
- [Configuraci√≥n de Cookies](./docs/CONFIGURACION_COOKIES.md)
- [Estructura del Proyecto](./ESTRUCTURA_PROYECTO.md)

## üîß Configuraci√≥n

### Aplicativo Web
- Ver `env.example.txt` para las variables de entorno necesarias
- Ver [Configuraci√≥n de Google Sheets API](./docs/CONFIGURACION_GOOGLE_SHEETS.md) para configurar la cuenta de servicio

### Scraper

El scraper procesa **un per√≠odo a la vez** para optimizar el tiempo de ejecuci√≥n y evitar timeouts.

#### Variables de Entorno Requeridas

```bash
# Credenciales de Google Sheets (JSON como string)
GOOGLE_CREDENTIALS='{"type":"service_account",...}'

# URLs de las hojas de Google Sheets
SHEET_URL_SOURCE="https://docs.google.com/spreadsheets/d/..."
SHEET_URL_TARGET="https://docs.google.com/spreadsheets/d/..."

# Per√≠odo objetivo a procesar (formato: YYYY-T, ej: "2026-1")
TARGET_PERIOD="2026-1"
```

#### Variables Opcionales

```bash
# Configuraci√≥n del scraper
UNIVALLE_BASE_URL="https://proxse26.univalle.edu.co/asignacion"
REQUEST_TIMEOUT="30"
REQUEST_MAX_RETRIES="3"
REQUEST_RETRY_DELAY="2"

# Cookies opcionales (si se requieren)
COOKIE_PHPSESSID=""
COOKIE_ASIGACAD=""

# Logging
LOG_LEVEL="INFO"
LOG_FILE="scraper.log"
```

#### Ejecuci√≥n Local

Para ejecutar el scraper localmente para un per√≠odo espec√≠fico:

```bash
# 1. Configurar variables de entorno
export GOOGLE_CREDENTIALS='{"type":"service_account",...}'
export SHEET_URL_SOURCE="https://docs.google.com/spreadsheets/d/..."
export SHEET_URL_TARGET="https://docs.google.com/spreadsheets/d/..."
export TARGET_PERIOD="2026-1"

# 2. Instalar dependencias
cd scraper
pip install -r requirements.txt

# 3. Ejecutar scraper
python main.py --modo completo \
  --source-sheet-url "$SHEET_URL_SOURCE" \
  --target-sheet-url "$SHEET_URL_TARGET" \
  --target-period "$TARGET_PERIOD" \
  --source-worksheet "2025-2" \
  --source-column "D" \
  --delay-cedulas 1.0
```

O usando solo la variable de entorno:

```bash
export TARGET_PERIOD="2026-1"
python scraper/main.py --modo completo
```

## ü§ñ GitHub Actions - Automatizaci√≥n del Scraper

El scraper est√° configurado con **9 workflows independientes**, uno para cada per√≠odo acad√©mico.

### Arquitectura de Workflows

Cada workflow procesa **un solo per√≠odo** de forma independiente:

- `scraper-2026-1.yml` - Per√≠odo 2026-1
- `scraper-2025-2.yml` - Per√≠odo 2025-2
- `scraper-2025-1.yml` - Per√≠odo 2025-1
- `scraper-2024-2.yml` - Per√≠odo 2024-2
- `scraper-2024-1.yml` - Per√≠odo 2024-1
- `scraper-2023-2.yml` - Per√≠odo 2023-2
- `scraper-2023-1.yml` - Per√≠odo 2023-1
- `scraper-2022-2.yml` - Per√≠odo 2022-2
- `scraper-2022-1.yml` - Per√≠odo 2022-1

### Ejecuci√≥n Autom√°tica

Los workflows se ejecutan autom√°ticamente todos los d√≠as con un **escalonamiento de 30 minutos** entre cada uno:

| Per√≠odo | Hora Colombia (COT) | Hora UTC | Cron Schedule |
|---------|---------------------|----------|---------------|
| 2026-1  | 3:00 AM            | 8:00 AM  | `0 8 * * *`   |
| 2025-2  | 3:30 AM            | 8:30 AM  | `30 8 * * *`  |
| 2025-1  | 4:00 AM            | 9:00 AM  | `0 9 * * *`   |
| 2024-2  | 4:30 AM            | 9:30 AM  | `30 9 * * *`  |
| 2024-1  | 5:00 AM            | 10:00 AM | `0 10 * * *`  |
| 2023-2  | 5:30 AM            | 10:30 AM | `30 10 * * *` |
| 2023-1  | 6:00 AM            | 11:00 AM | `0 11 * * *`  |
| 2022-2  | 6:30 AM            | 11:30 AM | `30 11 * * *` |
| 2022-1  | 7:00 AM            | 12:00 PM | `0 12 * * *`  |

**Tiempo total**: Aproximadamente 4.5 horas para procesar todos los per√≠odos (9 per√≠odos √ó 30 min de separaci√≥n)

### Ejecuci√≥n Manual

Para ejecutar manualmente un per√≠odo espec√≠fico:

1. Ve a **Actions** en GitHub
2. Selecciona el workflow del per√≠odo deseado (ej: "Scraper - Periodo 2026-1")
3. Haz clic en **Run workflow**
4. Opcionalmente ajusta los par√°metros:
   - `source_worksheet`: Hoja fuente (default: "2025-2")
   - `source_column`: Columna de c√©dulas (default: "D")
   - `delay_cedulas`: Delay entre c√©dulas en segundos (default: "1.0")

### Ventajas de la Arquitectura por Per√≠odo

‚úÖ **Sin timeouts**: Cada workflow tiene timeout de 45 minutos (suficiente para un per√≠odo)  
‚úÖ **Ejecuci√≥n independiente**: Si un per√≠odo falla, los dem√°s no se afectan  
‚úÖ **F√°cil troubleshooting**: Logs y artifacts espec√≠ficos por per√≠odo  
‚úÖ **Re-ejecuci√≥n selectiva**: Solo se re-ejecuta el per√≠odo que fall√≥  
‚úÖ **Escalonamiento**: Evita sobrecarga del sistema ejecutando en paralelo

### Troubleshooting

#### Si un per√≠odo falla:

1. **Re-ejecutar solo ese workflow**:
   - Ve a Actions > Selecciona el workflow del per√≠odo que fall√≥
   - Haz clic en "Re-run jobs" o "Run workflow"

2. **Revisar logs**:
   - Los logs se suben autom√°ticamente como artifacts si hay errores
   - Nombre del artifact: `scraper-logs-{PERIODO}-{RUN_NUMBER}-{ATTEMPT}`

3. **Verificar configuraci√≥n**:
   - Aseg√∫rate de que los secrets est√©n configurados:
     - `GOOGLE_CREDENTIALS`
     - `SHEET_URL_SOURCE`
     - `SHEET_URL_TARGET`

4. **Los dem√°s per√≠odos no se afectan**:
   - Cada workflow es completamente independiente
   - Un fallo en un per√≠odo no afecta la ejecuci√≥n de los otros

## üìù Notas

- ‚úÖ **El aplicativo web funciona sin necesidad de autenticaci√≥n con cookies** - El portal Univalle permite acceso p√∫blico
- ‚úÖ **Migraci√≥n completada**: `findDocentByPhone.html` y `searchState.gs` migrados a Next.js/React
- ‚úÖ **Web scraping funcional**: Extracci√≥n directa de datos desde el portal
- ‚úÖ **Scraper optimizado**: Procesamiento por per√≠odo individual para evitar timeouts
- El sistema de cosecha requiere configuraci√≥n de Google Sheets API
- Los archivos legacy se mantienen en `legacy/` como referencia

## üÜï Cambios Recientes (Enero 2025)

### Migraci√≥n a Next.js/React
- ‚úÖ Migrado `findDocentByPhone.html` ‚Üí Componentes React en `src/web/components/`
- ‚úÖ Migrado `searchState.gs` ‚Üí Servicios TypeScript en `src/web/lib/`
- ‚úÖ API Routes creadas en `app/api/` para per√≠odos y docentes
- ‚úÖ Web scraping funcional sin requerir cookies de autenticaci√≥n
- ‚úÖ Parser HTML mejorado con mejor detecci√≥n de errores
- ‚úÖ Procesamiento en paralelo de m√∫ltiples per√≠odos

### Refactorizaci√≥n del Scraper (Enero 2025)
- ‚úÖ **Arquitectura por per√≠odo individual**: Cada per√≠odo se procesa de forma independiente
- ‚úÖ **9 workflows independientes**: Un workflow por cada per√≠odo acad√©mico
- ‚úÖ **Ejecuci√≥n escalonada**: 30 minutos entre cada workflow para evitar sobrecarga
- ‚úÖ **Timeout optimizado**: 45 minutos por per√≠odo (vs 60 minutos anterior)
- ‚úÖ **Re-ejecuci√≥n selectiva**: Solo se re-ejecuta el per√≠odo que falla
- ‚úÖ **Variables de entorno simplificadas**: `TARGET_PERIOD` para especificar el per√≠odo objetivo

### Funcionalidades Implementadas
- B√∫squeda de docentes por c√©dula
- Visualizaci√≥n por per√≠odo y por actividad
- Extracci√≥n de datos desde portal Univalle
- Procesamiento de m√∫ltiples per√≠odos en paralelo
- Interfaz responsive con Bootstrap
- Scraper automatizado con GitHub Actions

## üìÑ Licencia

ISC

