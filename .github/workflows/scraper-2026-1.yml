name: Scraper - Periodo 2026-1

on:
  # Ejecutar autom√°ticamente a las 3:00 AM hora Colombia (UTC-5) todos los d√≠as
  # Colombia UTC-5, entonces 3:00 AM COT = 8:00 AM UTC
  schedule:
    - cron: '0 8 * * *'
  
  # Permite ejecuci√≥n manual desde GitHub Actions UI
  workflow_dispatch:
    inputs:
      source_worksheet:
        description: 'Hoja fuente (worksheet)'
        required: false
        default: '2025-2'
      source_column:
        description: 'Columna de c√©dulas'
        required: false
        default: 'D'
      delay_cedulas:
        description: 'Delay entre c√©dulas (segundos)'
        required: false
        default: '1.0'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Timeout de 45 minutos para un per√≠odo
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        working-directory: ./scraper
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create credentials.json from secret
        working-directory: ./scraper
        env:
          GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_CREDENTIALS }}
        run: |
          if [ -z "$GOOGLE_CREDENTIALS" ]; then
            echo "‚ùå Error: Secret GOOGLE_CREDENTIALS no est√° configurado"
            exit 1
          fi
          
          # Usar Python para escribir el JSON de forma segura
          # Esto maneja mejor caracteres especiales y validaci√≥n
          # Usamos 'EOF' para evitar expansi√≥n de variables en bash
          python3 << 'EOF'
          import json
          import os
          import sys
          
          credentials_str = os.environ.get('GOOGLE_CREDENTIALS', '')
          
          if not credentials_str:
              print("‚ùå Error: GOOGLE_CREDENTIALS est√° vac√≠o", file=sys.stderr)
              sys.exit(1)
          
          try:
              # Intentar parsear el JSON para validar que es v√°lido
              credentials_json = json.loads(credentials_str)
              
              # Escribir el JSON formateado al archivo
              with open('credentials.json', 'w', encoding='utf-8') as f:
                  json.dump(credentials_json, f, indent=2, ensure_ascii=False)
              
              print("‚úì JSON v√°lido y credentials.json creado exitosamente")
              
          except json.JSONDecodeError as e:
              print(f"‚ùå Error: El secret GOOGLE_CREDENTIALS no es un JSON v√°lido: {e}", file=sys.stderr)
              print(f"   L√≠nea {e.lineno}, columna {e.colno}: {e.msg}", file=sys.stderr)
              sys.exit(1)
          except Exception as e:
              print(f"‚ùå Error inesperado al crear credentials.json: {e}", file=sys.stderr)
              sys.exit(1)
          EOF
          
          # Verificar que el archivo se cre√≥ correctamente
          if [ ! -f credentials.json ] || [ ! -s credentials.json ]; then
            echo "‚ùå Error: credentials.json no se pudo crear o est√° vac√≠o"
            exit 1
          fi
          
          echo "‚úì Archivo credentials.json verificado"
      
      - name: Validate required secrets
        run: |
          if [ -z "${{ secrets.SHEET_URL_SOURCE }}" ]; then
            echo "‚ùå Error: Secret SHEET_URL_SOURCE no est√° configurado"
            exit 1
          fi
          
          if [ -z "${{ secrets.SHEET_URL_TARGET }}" ]; then
            echo "‚ùå Error: Secret SHEET_URL_TARGET no est√° configurado"
            exit 1
          fi
          
          echo "‚úì Secrets validados"
      
      - name: Extract spreadsheet IDs from URLs
        id: extract_ids
        run: |
          # Funci√≥n para extraer ID de una URL de Google Sheets
          extract_sheet_id() {
            local url="$1"
            # Extraer ID de URL: https://docs.google.com/spreadsheets/d/ID/edit
            echo "$url" | sed -n 's/.*\/spreadsheets\/d\/\([a-zA-Z0-9_-]\{44,\}\).*/\1/p'
          }
          
          SOURCE_URL="${{ secrets.SHEET_URL_SOURCE }}"
          TARGET_URL="${{ secrets.SHEET_URL_TARGET }}"
          
          # Extraer IDs
          SOURCE_ID=$(extract_sheet_id "$SOURCE_URL")
          TARGET_ID=$(extract_sheet_id "$TARGET_URL")
          
          # Si no se pudo extraer de la URL, intentar usar el valor directamente (por si ya es un ID)
          if [ -z "$SOURCE_ID" ]; then
            SOURCE_ID="$SOURCE_URL"
          fi
          if [ -z "$TARGET_ID" ]; then
            TARGET_ID="$TARGET_URL"
          fi
          
          # Validar que tenemos IDs
          if [ -z "$SOURCE_ID" ]; then
            echo "‚ùå Error: No se pudo extraer ID de SHEET_URL_SOURCE"
            exit 1
          fi
          if [ -z "$TARGET_ID" ]; then
            echo "‚ùå Error: No se pudo extraer ID de SHEET_URL_TARGET"
            exit 1
          fi
          
          echo "‚úì IDs extra√≠dos:"
          echo "  SOURCE_ID: ${SOURCE_ID:0:20}..."
          echo "  TARGET_ID: ${TARGET_ID:0:20}..."
          
          # Guardar en outputs para usar en el siguiente paso
          echo "source_id=$SOURCE_ID" >> $GITHUB_OUTPUT
          echo "target_id=$TARGET_ID" >> $GITHUB_OUTPUT
      
      - name: Run scraper
        working-directory: ./scraper
        env:
          # Credenciales de Google Sheets
          GOOGLE_SHEETS_CREDENTIALS_PATH: credentials.json
          
          # IDs de las hojas (extra√≠dos de las URLs)
          SHEET_SOURCE: ${{ steps.extract_ids.outputs.source_id }}
          SHEET_TARGET: ${{ steps.extract_ids.outputs.target_id }}
          
          # Tambi√©n configurar como GOOGLE_SHEETS_SOURCE_ID y GOOGLE_SHEETS_TARGET_ID
          GOOGLE_SHEETS_SOURCE_ID: ${{ steps.extract_ids.outputs.source_id }}
          GOOGLE_SHEETS_TARGET_ID: ${{ steps.extract_ids.outputs.target_id }}
          
          # Spreadsheet ID por defecto (legacy, usar SOURCE/TARGET en su lugar)
          GOOGLE_SHEETS_SPREADSHEET_ID: ${{ secrets.GOOGLE_SHEETS_SPREADSHEET_ID || steps.extract_ids.outputs.target_id }}
          
          # URLs de las hojas (desde secrets, para uso en comandos)
          SHEET_URL_SOURCE: ${{ secrets.SHEET_URL_SOURCE }}
          SHEET_URL_TARGET: ${{ secrets.SHEET_URL_TARGET }}
          
          # Configuraci√≥n del scraper (opcional, con defaults)
          UNIVALLE_BASE_URL: ${{ vars.UNIVALLE_BASE_URL || 'https://proxse26.univalle.edu.co/asignacion' }}
          REQUEST_TIMEOUT: ${{ vars.REQUEST_TIMEOUT || '30' }}
          REQUEST_MAX_RETRIES: ${{ vars.REQUEST_MAX_RETRIES || '3' }}
          REQUEST_RETRY_DELAY: ${{ vars.REQUEST_RETRY_DELAY || '2' }}
          DEFAULT_PERIODOS_COUNT: ${{ vars.DEFAULT_PERIODOS_COUNT || '8' }}
          
          # Per√≠odo objetivo a procesar
          TARGET_PERIOD: "2026-1"
          
          # Logging
          LOG_LEVEL: INFO
          LOG_FILE: scraper.log
          
          # Cookies opcionales (si est√°n configuradas)
          COOKIE_PHPSESSID: ${{ secrets.COOKIE_PHPSESSID || '' }}
          COOKIE_ASIGACAD: ${{ secrets.COOKIE_ASIGACAD || '' }}
        run: |
          set -e  # Salir en caso de error
          
          echo "üöÄ Iniciando scraper para per√≠odo 2026-1..."
          echo "   Per√≠odo objetivo: $TARGET_PERIOD"
          echo "   Hoja fuente: ${{ inputs.source_worksheet || '2025-2' }}"
          echo "   Columna: ${{ inputs.source_column || 'D' }}"
          
          python main.py \
            --modo completo \
            --source-sheet-url "$SHEET_URL_SOURCE" \
            --target-sheet-url "$SHEET_URL_TARGET" \
            --target-period "$TARGET_PERIOD" \
            --source-worksheet "${{ inputs.source_worksheet || '2025-2' }}" \
            --source-column "${{ inputs.source_column || 'D' }}" \
            --delay-cedulas ${{ inputs.delay_cedulas || '1.0' }}
        continue-on-error: true
        id: scrape_run
      
      - name: Upload logs on error or failure
        if: failure() || steps.scrape_run.outcome == 'failure'
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-2026-1-${{ github.run_number }}-${{ github.run_attempt }}
          path: |
            scraper/scraper.log
          retention-days: 7
          if-no-files-found: ignore
      
      - name: Cleanup credentials
        if: always()
        working-directory: ./scraper
        run: |
          if [ -f credentials.json ]; then
            # Eliminar archivo de credenciales (ya no se necesita)
            rm -f credentials.json
            echo "‚úì credentials.json eliminado por seguridad"
          fi
      
      - name: Check scraper exit status
        if: steps.scrape_run.outcome == 'failure'
        run: |
          echo "‚ùå El scraper fall√≥ con c√≥digo de salida no cero."
          echo "üìã Revisa los logs en los artifacts para m√°s detalles."
          if [ -f scraper/scraper.log ]; then
            echo ""
            echo "√öltimas 50 l√≠neas del log:"
            tail -50 scraper/scraper.log
          fi
          exit 1
      
      - name: Success summary
        if: success()
        run: |
          echo "‚úÖ Scraper ejecutado exitosamente para per√≠odo 2026-1"
          if [ -f scraper/scraper.log ]; then
            echo ""
            echo "Resumen del log:"
            grep -E "(RESUMEN FINAL|Tiempo total|C√©dulas|actividades)" scraper/scraper.log | tail -10 || echo "No se encontr√≥ resumen en el log"
          fi
