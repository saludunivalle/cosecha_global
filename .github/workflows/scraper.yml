name: Ejecutar Scraper Univalle

on:
  # Ejecutar autom√°ticamente con cron jobs separados para cada per√≠odo
  schedule:
    # 2:00 AM - Per√≠odo 2026-1
    - cron: '0 7 * * *'
    # 2:45 AM - Per√≠odo 2025-2  
    - cron: '45 7 * * *'
    # 3:30 AM - Per√≠odo 2025-1
    - cron: '30 8 * * *'
    # 4:15 AM - Per√≠odo 2024-2
    - cron: '15 9 * * *'
    # 5:00 AM - Per√≠odo 2024-1
    - cron: '0 10 * * *'
    # 5:45 AM - Per√≠odo 2023-2
    - cron: '45 10 * * *'
    # 6:30 AM - Per√≠odo 2023-1
    - cron: '30 11 * * *'
    # 7:15 AM - Per√≠odo 2022-2
    - cron: '15 12 * * *'
    # 8:00 AM - Per√≠odo 2022-1
    - cron: '0 13 * * *'
  
  # Permite ejecuci√≥n manual desde GitHub Actions UI
  workflow_dispatch:
    inputs:
      target_period:
        description: 'Per√≠odo objetivo a procesar (ej: 2026-1)'
        required: false
        default: '2026-1'
      source_worksheet:
        description: 'Hoja fuente (worksheet)'
        required: false
        default: '2025-2'
      source_column:
        description: 'Columna de c√©dulas'
        required: false
        default: 'D'
      delay_cedulas:
        description: 'Delay entre c√©dulas (segundos)'
        required: false
        default: '0.1'
      max_cedulas:
        description: 'M√°ximo de c√©dulas a procesar (dejar vac√≠o para todas)'
        required: false
        default: ''

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 40  # L√≠mite de 40 minutos por job
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        working-directory: ./scraper
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create credentials.json from secret
        working-directory: ./scraper
        env:
          GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_CREDENTIALS }}
        run: |
          if [ -z "$GOOGLE_CREDENTIALS" ]; then
            echo "‚ùå Error: Secret GOOGLE_CREDENTIALS no est√° configurado"
            exit 1
          fi
          
          python3 << 'EOF'
          import json
          import os
          import sys
          
          credentials_str = os.environ.get('GOOGLE_CREDENTIALS', '')
          
          if not credentials_str:
              print("‚ùå Error: GOOGLE_CREDENTIALS est√° vac√≠o", file=sys.stderr)
              sys.exit(1)
          
          try:
              credentials_json = json.loads(credentials_str)
              with open('credentials.json', 'w', encoding='utf-8') as f:
                  json.dump(credentials_json, f, indent=2, ensure_ascii=False)
              print("‚úì JSON v√°lido y credentials.json creado exitosamente")
          except json.JSONDecodeError as e:
              print(f"‚ùå Error: El secret GOOGLE_CREDENTIALS no es un JSON v√°lido: {e}", file=sys.stderr)
              sys.exit(1)
          except Exception as e:
              print(f"‚ùå Error inesperado al crear credentials.json: {e}", file=sys.stderr)
              sys.exit(1)
          EOF
          
          if [ ! -f credentials.json ] || [ ! -s credentials.json ]; then
            echo "‚ùå Error: credentials.json no se pudo crear o est√° vac√≠o"
            exit 1
          fi
          
          echo "‚úì Archivo credentials.json verificado"
      
      - name: Validate required secrets
        run: |
          if [ -z "${{ secrets.SHEET_URL_SOURCE }}" ]; then
            echo "‚ùå Error: Secret SHEET_URL_SOURCE no est√° configurado"
            exit 1
          fi
          
          if [ -z "${{ secrets.SHEET_URL_TARGET }}" ]; then
            echo "‚ùå Error: Secret SHEET_URL_TARGET no est√° configurado"
            exit 1
          fi
          
          echo "‚úì Secrets validados"
      
      - name: Extract spreadsheet IDs from URLs
        id: extract_ids
        run: |
          extract_sheet_id() {
            local url="$1"
            echo "$url" | sed -n 's/.*\/spreadsheets\/d\/\([a-zA-Z0-9_-]\{44,\}\).*/\1/p'
          }
          
          SOURCE_URL="${{ secrets.SHEET_URL_SOURCE }}"
          TARGET_URL="${{ secrets.SHEET_URL_TARGET }}"
          
          SOURCE_ID=$(extract_sheet_id "$SOURCE_URL")
          TARGET_ID=$(extract_sheet_id "$TARGET_URL")
          
          if [ -z "$SOURCE_ID" ]; then
            SOURCE_ID="$SOURCE_URL"
          fi
          if [ -z "$TARGET_ID" ]; then
            TARGET_ID="$TARGET_URL"
          fi
          
          if [ -z "$SOURCE_ID" ]; then
            echo "‚ùå Error: No se pudo extraer ID de SHEET_URL_SOURCE"
            exit 1
          fi
          if [ -z "$TARGET_ID" ]; then
            echo "‚ùå Error: No se pudo extraer ID de SHEET_URL_TARGET"
            exit 1
          fi
          
          echo "‚úì IDs extra√≠dos:"
          echo "  SOURCE_ID: ${SOURCE_ID:0:20}..."
          echo "  TARGET_ID: ${TARGET_ID:0:20}..."
          
          echo "source_id=$SOURCE_ID" >> $GITHUB_OUTPUT
          echo "target_id=$TARGET_ID" >> $GITHUB_OUTPUT
      
      - name: Determine which period to process
        id: determine_period
        run: |
          # En modo manual, usar el per√≠odo especificado
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            PERIOD="${{ inputs.target_period }}"
            echo "üìã Modo manual: procesando per√≠odo $PERIOD"
          else
            # En modo autom√°tico (cron), determinar per√≠odo por horario
            HOUR=$(date -u +%H)
            case "$HOUR" in
              7)  PERIOD="2026-1" ;;
              8)  PERIOD="2025-2" ;;
              9)  PERIOD="2024-2" ;;
              10) PERIOD="2024-1" ;;
              11) PERIOD="2023-2" ;;
              12) PERIOD="2022-2" ;;
              13) PERIOD="2022-1" ;;
              *)  
                # Para otros horarios, determinar por minuto tambi√©n
                MINUTE=$(date -u +%M)
                if [ "$HOUR" -eq 7 ] && [ "$MINUTE" -ge 45 ]; then
                  PERIOD="2025-2"
                elif [ "$HOUR" -eq 8 ] && [ "$MINUTE" -ge 30 ]; then
                  PERIOD="2025-1"
                elif [ "$HOUR" -eq 9 ] && [ "$MINUTE" -ge 15 ]; then
                  PERIOD="2024-2"
                elif [ "$HOUR" -eq 10 ] && [ "$MINUTE" -ge 45 ]; then
                  PERIOD="2023-2"
                elif [ "$HOUR" -eq 11 ] && [ "$MINUTE" -ge 30 ]; then
                  PERIOD="2023-1"
                elif [ "$HOUR" -eq 12 ] && [ "$MINUTE" -ge 15 ]; then
                  PERIOD="2022-2"
                else
                  PERIOD="2026-1"
                fi
                ;;
            esac
            echo "üìã Modo autom√°tico: procesando per√≠odo $PERIOD (hora UTC: $HOUR)"
          fi
          
          echo "period=$PERIOD" >> $GITHUB_OUTPUT
          echo "‚úì Per√≠odo seleccionado: $PERIOD"
      
      - name: Run scraper for single period
        working-directory: ./scraper
        timeout-minutes: 38  # Dejar 2 minutos para cleanup
        env:
          GOOGLE_SHEETS_CREDENTIALS_PATH: credentials.json
          SHEET_SOURCE: ${{ steps.extract_ids.outputs.source_id }}
          SHEET_TARGET: ${{ steps.extract_ids.outputs.target_id }}
          GOOGLE_SHEETS_SOURCE_ID: ${{ steps.extract_ids.outputs.source_id }}
          GOOGLE_SHEETS_TARGET_ID: ${{ steps.extract_ids.outputs.target_id }}
          GOOGLE_SHEETS_SPREADSHEET_ID: ${{ secrets.GOOGLE_SHEETS_SPREADSHEET_ID || steps.extract_ids.outputs.target_id }}
          SHEET_URL_SOURCE: ${{ secrets.SHEET_URL_SOURCE }}
          SHEET_URL_TARGET: ${{ secrets.SHEET_URL_TARGET }}
          UNIVALLE_BASE_URL: ${{ vars.UNIVALLE_BASE_URL || 'https://proxse26.univalle.edu.co/asignacion' }}
          REQUEST_TIMEOUT: ${{ vars.REQUEST_TIMEOUT || '60' }}
          REQUEST_MAX_RETRIES: ${{ vars.REQUEST_MAX_RETRIES || '5' }}
          REQUEST_RETRY_DELAY: ${{ vars.REQUEST_RETRY_DELAY || '3' }}
          DEFAULT_PERIODOS_COUNT: ${{ vars.DEFAULT_PERIODOS_COUNT || '8' }}
          SHEETS_READ_TIMEOUT: ${{ vars.SHEETS_READ_TIMEOUT || '120' }}
          SHEETS_MAX_RETRIES: ${{ vars.SHEETS_MAX_RETRIES || '5' }}
          SHEETS_RETRY_DELAY: ${{ vars.SHEETS_RETRY_DELAY || '10' }}
          SHEETS_BATCH_SIZE: ${{ vars.SHEETS_BATCH_SIZE || '100' }}
          LOG_LEVEL: INFO
          LOG_FILE: scraper.log
          COOKIE_PHPSESSID: ${{ secrets.COOKIE_PHPSESSID || '' }}
          COOKIE_ASIGACAD: ${{ secrets.COOKIE_ASIGACAD || '' }}
        run: |
          set +e
          
          PERIOD="${{ steps.determine_period.outputs.period }}"
          
          echo "=========================================================================="
          echo "üöÄ PROCESANDO PER√çODO: $PERIOD"
          echo "=========================================================================="
          echo "   Hora inicio: $(date '+%Y-%m-%d %H:%M:%S %Z')"
          echo "   Hoja fuente: ${{ inputs.source_worksheet || '2025-2' }}"
          echo "   Columna: ${{ inputs.source_column || 'D' }}"
          echo "   Delay entre c√©dulas: ${{ inputs.delay_cedulas || '0.1' }}s"
          echo "   M√°ximo c√©dulas: ${{ inputs.max_cedulas || 'todas' }}"
          echo ""
          
          # Iniciar keep-alive
          (
            while true; do
              sleep 300
              echo "   [Keep-Alive] Procesando... ($(date '+%H:%M:%S'))"
            done
          ) &
          KEEPALIVE_PID=$!
          
          # Ejecutar scraper
          START_TIME=$(date +%s)
          
          # Construir comando con par√°metros opcionales
          CMD="python main.py --modo completo"
          CMD="$CMD --source-sheet-url \"$SHEET_URL_SOURCE\""
          CMD="$CMD --target-sheet-url \"$SHEET_URL_TARGET\""
          CMD="$CMD --target-period \"$PERIOD\""
          CMD="$CMD --source-worksheet \"${{ inputs.source_worksheet || '2025-2' }}\""
          CMD="$CMD --source-column \"${{ inputs.source_column || 'D' }}\""
          CMD="$CMD --delay-cedulas ${{ inputs.delay_cedulas || '0.1' }}"
          
          # Agregar max-cedulas si est√° especificado
          if [ -n "${{ inputs.max_cedulas }}" ]; then
            CMD="$CMD --max-cedulas ${{ inputs.max_cedulas }}"
            echo "‚ö†Ô∏è  L√çMITE: Procesando m√°ximo ${{ inputs.max_cedulas }} c√©dulas"
          fi
          
          echo "Ejecutando: $CMD"
          echo ""
          
          if eval $CMD; then
            
            kill $KEEPALIVE_PID 2>/dev/null || true
            wait $KEEPALIVE_PID 2>/dev/null || true
            
            END_TIME=$(date +%s)
            DURATION=$(((END_TIME - START_TIME) / 60))
            
            echo ""
            echo "‚úÖ Per√≠odo $PERIOD completado exitosamente en $DURATION minutos"
            echo "   Hora fin: $(date '+%Y-%m-%d %H:%M:%S %Z')"
            exit 0
          else
            kill $KEEPALIVE_PID 2>/dev/null || true
            wait $KEEPALIVE_PID 2>/dev/null || true
            
            echo ""
            echo "‚ùå Error al procesar per√≠odo $PERIOD"
            echo "   Hora fin: $(date '+%Y-%m-%d %H:%M:%S %Z')"
            exit 1
          fi
        continue-on-error: true
        id: scrape_run
      
      - name: Upload logs on error or failure
        if: failure() || steps.scrape_run.outcome == 'failure'
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ steps.determine_period.outputs.period }}-${{ github.run_number }}
          path: |
            scraper/scraper.log
          retention-days: 7
          if-no-files-found: ignore
      
      - name: Cleanup credentials
        if: always()
        working-directory: ./scraper
        run: |
          if [ -f credentials.json ]; then
            rm -f credentials.json
            echo "‚úì credentials.json eliminado"
          fi
