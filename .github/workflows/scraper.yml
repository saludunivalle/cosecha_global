name: Ejecutar Scraper Univalle

on:
  # Ejecutar autom√°ticamente a las 3:00 AM hora Colombia (UTC-5) todos los d√≠as
  # Colombia UTC-5, entonces 3:00 AM COT = 8:00 AM UTC
  schedule:
    - cron: '0 8 * * *'
  
  # Permite ejecuci√≥n manual desde GitHub Actions UI
  workflow_dispatch:
    inputs:
      current_period:
        description: 'Per√≠odo actual (ej: 2026-1)'
        required: false
        default: '2026-1'
      n_periodos:
        description: 'N√∫mero de per√≠odos anteriores'
        required: false
        default: '8'
      source_worksheet:
        description: 'Hoja fuente (worksheet)'
        required: false
        default: '2025-2'
      source_column:
        description: 'Columna de c√©dulas'
        required: false
        default: 'D'
      delay_cedulas:
        description: 'Delay entre c√©dulas (segundos)'
        required: false
        default: '1.0'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Timeout de 1 hora para el job completo
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        working-directory: ./scraper
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create credentials.json from secret
        working-directory: ./scraper
        run: |
          if [ -z "${{ secrets.GOOGLE_CREDENTIALS }}" ]; then
            echo "‚ùå Error: Secret GOOGLE_CREDENTIALS no est√° configurado"
            exit 1
          fi
          
          echo "${{ secrets.GOOGLE_CREDENTIALS }}" > credentials.json
          
          # Verificar que el archivo se cre√≥ correctamente y es JSON v√°lido
          if [ ! -f credentials.json ] || [ ! -s credentials.json ]; then
            echo "‚ùå Error: credentials.json no se pudo crear o est√° vac√≠o"
            exit 1
          fi
          
          # Verificar que es JSON v√°lido
          if ! python -c "import json; json.load(open('credentials.json'))" 2>/dev/null; then
            echo "‚ö†Ô∏è  Advertencia: credentials.json podr√≠a no ser JSON v√°lido"
            echo "El workflow continuar√° pero podr√≠a fallar m√°s adelante"
          else
            echo "‚úì JSON v√°lido"
          fi
          
          echo "‚úì credentials.json creado exitosamente"
      
      - name: Validate required secrets
        run: |
          if [ -z "${{ secrets.SHEET_URL_SOURCE }}" ]; then
            echo "‚ùå Error: Secret SHEET_URL_SOURCE no est√° configurado"
            exit 1
          fi
          
          if [ -z "${{ secrets.SHEET_URL_TARGET }}" ]; then
            echo "‚ùå Error: Secret SHEET_URL_TARGET no est√° configurado"
            exit 1
          fi
          
          echo "‚úì Secrets validados"
      
      - name: Run scraper
        working-directory: ./scraper
        env:
          # Credenciales de Google Sheets
          GOOGLE_SHEETS_CREDENTIALS_PATH: credentials.json
          
          # Spreadsheet ID por defecto (puede extraerse de la URL target si es necesario)
          # Si el scraper usa URLs directamente, esto puede estar vac√≠o
          GOOGLE_SHEETS_SPREADSHEET_ID: ${{ secrets.GOOGLE_SHEETS_SPREADSHEET_ID || '' }}
          
          # URLs de las hojas (desde secrets)
          SHEET_URL_SOURCE: ${{ secrets.SHEET_URL_SOURCE }}
          SHEET_URL_TARGET: ${{ secrets.SHEET_URL_TARGET }}
          
          # Configuraci√≥n del scraper (opcional, con defaults)
          UNIVALLE_BASE_URL: ${{ vars.UNIVALLE_BASE_URL || 'https://proxse26.univalle.edu.co/asignacion' }}
          REQUEST_TIMEOUT: ${{ vars.REQUEST_TIMEOUT || '30' }}
          REQUEST_MAX_RETRIES: ${{ vars.REQUEST_MAX_RETRIES || '3' }}
          REQUEST_RETRY_DELAY: ${{ vars.REQUEST_RETRY_DELAY || '2' }}
          DEFAULT_PERIODOS_COUNT: ${{ vars.DEFAULT_PERIODOS_COUNT || '8' }}
          
          # Logging
          LOG_LEVEL: INFO
          LOG_FILE: scraper.log
          
          # Cookies opcionales (si est√°n configuradas)
          COOKIE_PHPSESSID: ${{ secrets.COOKIE_PHPSESSID || '' }}
          COOKIE_ASIGACAD: ${{ secrets.COOKIE_ASIGACAD || '' }}
        run: |
          set -e  # Salir en caso de error
          
          echo "üöÄ Iniciando scraper..."
          echo "   Per√≠odo actual: ${{ inputs.current_period || '2026-1' }}"
          echo "   N√∫mero de per√≠odos: ${{ inputs.n_periodos || '8' }}"
          echo "   Hoja fuente: ${{ inputs.source_worksheet || '2025-2' }}"
          echo "   Columna: ${{ inputs.source_column || 'D' }}"
          
          python main.py \
            --modo completo \
            --source-sheet-url "$SHEET_URL_SOURCE" \
            --target-sheet-url "$SHEET_URL_TARGET" \
            --current-period "${{ inputs.current_period || '2026-1' }}" \
            --n-periodos ${{ inputs.n_periodos || '8' }} \
            --source-worksheet "${{ inputs.source_worksheet || '2025-2' }}" \
            --source-column "${{ inputs.source_column || 'D' }}" \
            --delay-cedulas ${{ inputs.delay_cedulas || '1.0' }}
        continue-on-error: true
        id: scrape_run
      
      - name: Upload logs on error or failure
        if: failure() || steps.scrape_run.outcome == 'failure'
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}-${{ github.run_attempt }}
          path: |
            scraper/scraper.log
          retention-days: 7
          if-no-files-found: ignore
      
      - name: Cleanup credentials
        if: always()
        working-directory: ./scraper
        run: |
          if [ -f credentials.json ]; then
            # Eliminar archivo de credenciales (ya no se necesita)
            rm -f credentials.json
            echo "‚úì credentials.json eliminado por seguridad"
          fi
      
      - name: Check scraper exit status
        if: steps.scrape_run.outcome == 'failure'
        run: |
          echo "‚ùå El scraper fall√≥ con c√≥digo de salida no cero."
          echo "üìã Revisa los logs en los artifacts para m√°s detalles."
          if [ -f scraper/scraper.log ]; then
            echo ""
            echo "√öltimas 50 l√≠neas del log:"
            tail -50 scraper/scraper.log
          fi
          exit 1
      
      - name: Success summary
        if: success()
        run: |
          echo "‚úÖ Scraper ejecutado exitosamente"
          if [ -f scraper/scraper.log ]; then
            echo ""
            echo "Resumen del log:"
            grep -E "(RESUMEN FINAL|Tiempo total|C√©dulas|actividades)" scraper/scraper.log | tail -10 || echo "No se encontr√≥ resumen en el log"
          fi

