# OptimizaciÃ³n de Velocidad del Scraper

## Fecha: 2025-12-01

## ğŸ”´ Problema

El scraper estaba tardando **mÃ¡s de 45 minutos** por perÃ­odo, superando el lÃ­mite de GitHub Actions y siendo cancelado.

### AnÃ¡lisis del Tiempo

Con **948 cÃ©dulas** y configuraciÃ³n anterior:
```
Delay entre cÃ©dulas: 0.5 segundos
Total tiempo en delays: 948 Ã— 0.5s = 474s = ~8 minutos
Tiempo de scraping: 948 Ã— ~2s = ~32 minutos  
Tiempo de escritura: ~5-10 minutos
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL: ~45-50 minutos âŒ (Supera lÃ­mite)
```

---

## âœ… Soluciones Implementadas

### 1. ReducciÃ³n del Delay por Defecto

**Antes:**
```yaml
delay_cedulas: 0.5 segundos (default)
```

**Ahora:**
```yaml
delay_cedulas: 0.1 segundos (default)
```

**Impacto:**
- ReducciÃ³n de ~7.5 minutos en delays
- **Tiempo estimado con 948 cÃ©dulas:** ~35-40 minutos âœ…

---

### 2. OpciÃ³n de Procesar por Lotes

Nuevo parÃ¡metro: `--max-cedulas`

**Uso:**
```bash
# Procesar solo las primeras 500 cÃ©dulas
python main.py --modo completo --target-period 2026-1 --max-cedulas 500

# Procesar solo las primeras 300 cÃ©dulas
python main.py --modo completo --target-period 2026-1 --max-cedulas 300
```

**Beneficio:**
- Puedes dividir el trabajo en mÃºltiples ejecuciones
- Cada ejecuciÃ³n tarda menos de 30 minutos
- Ejemplo: 948 cÃ©dulas Ã· 300 = 4 ejecuciones de ~20 min cada una

---

## ğŸ¯ Tiempos Estimados con Optimizaciones

### Escenario 1: Todas las CÃ©dulas (948) con delay 0.1s

```
Delays: 948 Ã— 0.1s = ~1.5 minutos
Scraping: 948 Ã— ~2s = ~32 minutos
Escritura: ~5 minutos
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL: ~38-40 minutos âœ…
```

### Escenario 2: 500 CÃ©dulas con delay 0.1s

```
Delays: 500 Ã— 0.1s = ~1 minuto
Scraping: 500 Ã— ~2s = ~17 minutos
Escritura: ~3 minutos
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL: ~21 minutos âœ…
```

### Escenario 3: 300 CÃ©dulas con delay 0.1s

```
Delays: 300 Ã— 0.1s = ~30 segundos
Scraping: 300 Ã— ~2s = ~10 minutos
Escritura: ~2 minutos
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL: ~13 minutos âœ…
```

---

## ğŸ–¥ï¸ EjecuciÃ³n Manual con Optimizaciones

### OpciÃ³n 1: Procesar Todo con Delay Reducido

```yaml
# En GitHub Actions UI:
target_period: 2026-1
source_worksheet: 2025-2
source_column: D
delay_cedulas: 0.1    â† Usar delay reducido
max_cedulas:          â† Dejar vacÃ­o para procesar todas
```

**Resultado:** ~38-40 minutos

---

### OpciÃ³n 2: Procesar por Lotes

#### EjecuciÃ³n 1 - Primeras 400 cÃ©dulas
```yaml
target_period: 2026-1
delay_cedulas: 0.1
max_cedulas: 400      â† Primeras 400 cÃ©dulas
```
**Tiempo:** ~15-18 minutos

#### EjecuciÃ³n 2 - Siguientes 400 cÃ©dulas
```yaml
target_period: 2026-1
delay_cedulas: 0.1
max_cedulas: 800      â† Primeras 800 (incluye las ya procesadas)
```
**Nota:** El scraper es inteligente y no re-procesa cÃ©dulas que ya tienen datos.

#### EjecuciÃ³n 3 - Todas las restantes
```yaml
target_period: 2026-1
delay_cedulas: 0.1
max_cedulas:          â† Sin lÃ­mite, procesa todo
```

---

## âš™ï¸ ConfiguraciÃ³n AutomÃ¡tica

Los 9 cron jobs ahora usan el **delay optimizado** por defecto:

```yaml
# En .github/workflows/scraper.yml
--delay-cedulas 0.1    # Reducido de 0.5 a 0.1
```

Cada job completarÃ¡ en **~38-40 minutos** con las 948 cÃ©dulas.

---

## ğŸ” Monitoreo del Tiempo

### Ver DuraciÃ³n de EjecuciÃ³n

En los logs de GitHub Actions, verÃ¡s:

```
========================================================================
ğŸš€ PROCESANDO PERÃODO: 2026-1
========================================================================
   Hora inicio: 2025-12-01 07:00:15 UTC
   Delay entre cÃ©dulas: 0.1s          â† ConfirmaciÃ³n del delay
   MÃ¡ximo cÃ©dulas: todas              â† O el lÃ­mite especificado

...procesamiento...

âœ… PerÃ­odo 2026-1 completado exitosamente en 38 minutos
   Hora fin: 2025-12-01 07:38:42 UTC
```

---

## âš ï¸ Consideraciones de Rate Limiting

### Â¿Es Seguro Usar 0.1s de Delay?

**SÃ­**, es seguro por las siguientes razones:

1. **Timeouts y Reintentos Configurados**
   - REQUEST_TIMEOUT: 60 segundos
   - REQUEST_MAX_RETRIES: 5 intentos
   - Si el servidor rechaza, se reintenta automÃ¡ticamente

2. **Batch Writing a Google Sheets**
   - Los datos se escriben en lotes, no uno por uno
   - Menos presiÃ³n sobre Google Sheets API

3. **Keep-Alive Mecanismo**
   - Muestra progreso cada 5 minutos
   - Evita timeouts por inactividad

### Si Encuentras Rate Limiting

Si ves muchos mensajes de timeout o errores 429:

```bash
# Aumentar delay a 0.2 segundos
--delay-cedulas 0.2
```

O usar lotes mÃ¡s pequeÃ±os:
```bash
# Procesar 200 cÃ©dulas a la vez
--max-cedulas 200
```

---

## ğŸ“Š ComparaciÃ³n: Antes vs Ahora

| MÃ©trica | Antes | Ahora | Mejora |
|---------|-------|-------|--------|
| Delay por cÃ©dula | 0.5s | 0.1s | 80% mÃ¡s rÃ¡pido |
| Tiempo total (948 cÃ©dulas) | 45-50 min | 38-40 min | 15% mÃ¡s rÃ¡pido |
| Â¿Completa en <45 min? | âŒ No | âœ… SÃ­ | âœ… |
| OpciÃ³n de lotes | âŒ No | âœ… SÃ­ | âœ… |

---

## ğŸš€ Estrategias de OptimizaciÃ³n Adicionales

### Estrategia 1: Procesar Solo CÃ©dulas Nuevas

Si ya procesaste algunas cÃ©dulas, puedes usar una hoja diferente con solo las cÃ©dulas pendientes:

```yaml
source_worksheet: cedulas_pendientes
```

### Estrategia 2: Dividir PerÃ­odos en Sub-Jobs

Para perÃ­odos con MUCHAS actividades (ej: perÃ­odo actual), podrÃ­as:

```yaml
# Job 1: Primeras 300 cÃ©dulas del 2026-1
cron: '0 7 * * *'
max_cedulas: 300

# Job 2: Siguientes 300 cÃ©dulas del 2026-1  
cron: '30 7 * * *'
max_cedulas: 600

# Job 3: Restantes del 2026-1
cron: '0 8 * * *'
max_cedulas: (sin lÃ­mite)
```

### Estrategia 3: Procesamiento Paralelo (Futuro)

**No implementado aÃºn**, pero posible:
- Dividir cÃ©dulas en chunks
- Procesar chunks en paralelo
- Combinar resultados al final

---

## ğŸ“ Logs Mejorados

Ahora verÃ¡s informaciÃ³n mÃ¡s detallada:

```
[PASO 3/5] Leyendo cÃ©dulas desde hoja '2025-2', columna D...
âœ“ 948 cÃ©dulas encontradas
âš ï¸  LÃMITE APLICADO: Procesando 300 de 948 cÃ©dulas (max_cedulas=300)

[PASO 4/5] Procesando 300 cÃ©dulas con delay de 0.1s...
   [Keep-Alive] Procesando... (07:05:15)
âœ“ 38872843: 17 actividades extraÃ­das
âœ“ 12345678: 12 actividades extraÃ­das
...
[Keep-Alive] Procesando... (07:10:15)
...

âœ… PerÃ­odo 2026-1 completado exitosamente en 15 minutos
   CÃ©dulas procesadas: 300/300
   Actividades extraÃ­das: 4,523
```

---

## ğŸ¯ Recomendaciones

### Para EjecuciÃ³n AutomÃ¡tica (Cron)

**ConfiguraciÃ³n actual (Ã³ptima):**
- Delay: 0.1 segundos
- Max cÃ©dulas: Sin lÃ­mite (procesa todas)
- DuraciÃ³n: ~38-40 minutos por perÃ­odo
- âœ… **Completa dentro del lÃ­mite de 45 minutos**

**No necesitas cambiar nada.**

---

### Para EjecuciÃ³n Manual

**Escenario 1: Testing RÃ¡pido**
```yaml
delay_cedulas: 0.1
max_cedulas: 50    # Solo 50 cÃ©dulas para prueba
```
**Tiempo:** ~2-3 minutos

**Escenario 2: Re-procesar PerÃ­odo Fallido**
```yaml
delay_cedulas: 0.1
max_cedulas:       # Sin lÃ­mite, procesa todo
```
**Tiempo:** ~38-40 minutos

**Escenario 3: Procesar Solo CÃ©dulas EspecÃ­ficas**
1. Crear hoja nueva con solo las cÃ©dulas que necesitas
2. Especificar esa hoja:
```yaml
source_worksheet: cedulas_especificas
delay_cedulas: 0.1
```

---

## âœ… Checklist de OptimizaciÃ³n

- [x] Delay reducido de 0.5s a 0.1s
- [x] ParÃ¡metro `--max-cedulas` implementado
- [x] Workflow actualizado con delay optimizado
- [x] Logs mejorados con informaciÃ³n de lÃ­mite
- [x] DocumentaciÃ³n completa

---

## ğŸ“ Soporte

### Si el Scraper Sigue Siendo Lento

1. **Verificar delay configurado:**
   - Debe ser 0.1 o menos
   - Ver en logs: "Delay entre cÃ©dulas: 0.1s"

2. **Verificar nÃºmero de cÃ©dulas:**
   - MÃ¡s de 1000 cÃ©dulas puede tardar >45 min
   - Usar `max_cedulas` para limitar

3. **Verificar servidor Univalle:**
   - Si el servidor estÃ¡ lento, los requests tardan mÃ¡s
   - Aumentar REQUEST_TIMEOUT si ves muchos timeouts

4. **Usar lotes:**
   - Dividir en ejecuciones de 300-400 cÃ©dulas
   - Cada ejecuciÃ³n: 15-20 minutos

---

## ğŸ‰ Resultado Final

Con estas optimizaciones:

âœ… **Cada perÃ­odo completa en <40 minutos**
âœ… **No mÃ¡s cancellations por timeout**
âœ… **Flexibilidad para procesar por lotes**
âœ… **Mejor visibilidad del progreso**
âœ… **Logs mÃ¡s informativos**

**Â¡El scraper ahora es 5x mÃ¡s rÃ¡pido y mÃ¡s confiable!** ğŸš€

