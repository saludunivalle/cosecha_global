# Soluci√≥n a Timeouts en Ejecuci√≥n del Scraper

## Fecha: 2025-11-30

## üî¥ Problema Identificado

El scraper se deten√≠a despu√©s de ~45 minutos de ejecuci√≥n, incluso cuando hab√≠a m√°s c√©dulas por procesar.

### Causas Identificadas

1. **Timeout del Step**: GitHub Actions tiene timeout por defecto de 360 minutos (6 horas) para el job completo, pero cada step puede tener su propio timeout
2. **Idle Timeout**: Si no hay output durante mucho tiempo, GitHub Actions puede detener el proceso
3. **Timeouts de Request**: Los timeouts de HTTP eran muy cortos (30 segundos)
4. **Timeouts de Google Sheets**: Timeout de 60 segundos era insuficiente para operaciones grandes

---

## ‚úÖ Soluciones Implementadas

### 1. Timeout del Step Aumentado

```yaml
- name: Run scraper for all periods
  timeout-minutes: 180  # 3 horas por per√≠odo
```

**Beneficio**: Cada per√≠odo individual puede ejecutarse hasta 3 horas sin ser interrumpido.

---

### 2. Keep-Alive en Background

```bash
# Iniciar keep-alive en background
(
  while true; do
    sleep 300  # 5 minutos
    echo "   [Keep-Alive] Procesando... ($(date '+%H:%M:%S'))"
  done
) &
KEEPALIVE_PID=$!

# Ejecutar scraper
python main.py ...

# Detener keep-alive al finalizar
kill $KEEPALIVE_PID 2>/dev/null || true
```

**Beneficio**: 
- Imprime mensaje cada 5 minutos
- Evita que GitHub Actions piense que el proceso est√° inactivo
- Se detiene autom√°ticamente cuando termina el per√≠odo

---

### 3. Timeout del Comando Python

```bash
timeout 7200 python main.py ...  # 2 horas m√°ximo
```

**Beneficio**: 
- L√≠mite de seguridad de 2 horas por per√≠odo
- Si el scraper se cuelga, se detiene y contin√∫a con el siguiente per√≠odo
- No afecta toda la ejecuci√≥n

---

### 4. Timeouts de HTTP Aumentados

#### Requests a Univalle

```yaml
REQUEST_TIMEOUT: 60 segundos      # Antes: 30s
REQUEST_MAX_RETRIES: 5            # Antes: 3
REQUEST_RETRY_DELAY: 3 segundos   # Antes: 2s
```

#### Google Sheets API

```yaml
SHEETS_READ_TIMEOUT: 120 segundos  # Antes: 60s
SHEETS_MAX_RETRIES: 5              # Antes: 3
SHEETS_RETRY_DELAY: 10 segundos    # Antes: 5s
SHEETS_BATCH_SIZE: 100             # Lotes m√°s peque√±os
```

**Beneficio**:
- Permite que requests lentas completen
- M√°s reintentos ante fallos temporales
- Mejor manejo de redes lentas o sobrecargadas

---

### 5. Progreso Visible Durante Delays

```bash
# Mostrar progreso cada 5 minutos
for min in {1..40}; do
  sleep 60
  if [ $((min % 5)) -eq 0 ]; then
    echo "   ... $((40 - min)) minutos restantes hasta siguiente per√≠odo"
  fi
done
```

**Beneficio**:
- Output visible cada 5 minutos
- Evita idle timeout durante los delays de 40 minutos
- Permite monitorear que el proceso sigue activo

---

## üìä Jerarqu√≠a de Timeouts

```
Job Timeout: 14 horas (840 min)
   ‚îî‚îÄ> Step Timeout: 3 horas (180 min) por per√≠odo
         ‚îî‚îÄ> Command Timeout: 2 horas (120 min) por per√≠odo
               ‚îî‚îÄ> Request Timeout: 60 segundos por request HTTP
               ‚îî‚îÄ> Sheets Timeout: 120 segundos por operaci√≥n Sheets
```

---

## üîç Tiempos Estimados por Per√≠odo

### Escenario Conservador (muchas c√©dulas)
```
Lectura de c√©dulas:          ~2-5 minutos
Scraping (948 c√©dulas):      ~60-90 minutos
  - 1 segundo por c√©dula     = ~16 minutos
  - Delays entre requests    = ~20-30 minutos
  - Procesamiento y escritura = ~30-40 minutos
Delay hasta siguiente:       40 minutos

Total por per√≠odo:           ~100-135 minutos
```

### Escenario Optimista (pocas c√©dulas)
```
Total por per√≠odo:           ~30-45 minutos
```

---

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Aumentar Timeout de un Per√≠odo Espec√≠fico

Si necesitas m√°s de 3 horas para un per√≠odo (poco probable), edita:

```yaml
timeout-minutes: 240  # 4 horas
```

### Ajustar Keep-Alive Frequency

Si quieres m√°s/menos mensajes de keep-alive:

```bash
# M√°s frecuente (cada 2 minutos)
sleep 120

# Menos frecuente (cada 10 minutos)
sleep 600
```

### Ajustar Timeout del Comando

```bash
# Aumentar a 3 horas
timeout 10800 python main.py ...

# Sin timeout (no recomendado)
python main.py ...
```

---

## üö® Se√±ales de Problemas

### El scraper sigue deteni√©ndose despu√©s de 3 horas

**Causa probable**: El step timeout (180 min) est√° siendo alcanzado.

**Soluci√≥n**: 
```yaml
timeout-minutes: 300  # Aumentar a 5 horas
```

### No hay output durante mucho tiempo

**Causa probable**: El keep-alive no est√° funcionando o el script est√° colgado.

**Verificaci√≥n**:
1. Busca mensajes `[Keep-Alive]` en los logs cada 5 minutos
2. Si no aparecen, el keep-alive muri√≥ prematuramente

**Soluci√≥n**: Verificar que el script de keep-alive est√© correcto.

### Muchos timeouts de requests HTTP

**Causa probable**: El servidor Univalle est√° lento o sobrecargado.

**Soluci√≥n temporal**:
```yaml
REQUEST_TIMEOUT: 90  # Aumentar a 90 segundos
```

### Timeouts de Google Sheets API

**Causa probable**: Operaciones muy grandes o red lenta.

**Soluci√≥n**:
```yaml
SHEETS_READ_TIMEOUT: 180      # 3 minutos
SHEETS_BATCH_SIZE: 50         # Lotes m√°s peque√±os
```

---

## üìù Logs a Monitorear

### Output Normal

```
üöÄ PER√çODO 1/9: 2026-1
   Hora inicio: 2025-11-30 07:00:15 UTC
   Hoja fuente: 2025-2
   Columna: D

‚úì 38872843: 17 actividades extra√≠das
   [Keep-Alive] Procesando... (07:05:15)
‚úì 12345678: 12 actividades extra√≠das
   [Keep-Alive] Procesando... (07:10:15)
...
‚úÖ Per√≠odo 2026-1 completado exitosamente en 45 minutos
```

### Output con Problemas

```
‚úì 38872843: 17 actividades extra√≠das
   [Keep-Alive] Procesando... (07:05:15)
‚ùå Error al procesar c√©dula 12345678: ReadTimeout
   [Keep-Alive] Procesando... (07:10:15)
‚ö†Ô∏è Reintentando c√©dula 12345678 (intento 2/5)...
‚úì 12345678: 12 actividades extra√≠das
```

---

## üîß Troubleshooting

### El keep-alive no aparece

```bash
# Verificar que el proceso est√° corriendo
ps aux | grep "sleep 300"

# Ver logs del keep-alive
grep "Keep-Alive" scraper.log
```

### El timeout del comando se alcanza

```bash
# Ver cu√°nto tiempo tard√≥ cada per√≠odo
grep "completado exitosamente en" scraper.log
```

**Ejemplo de output**:
```
‚úÖ Per√≠odo 2026-1 completado exitosamente en 45 minutos
‚úÖ Per√≠odo 2025-2 completado exitosamente en 52 minutos
‚úÖ Per√≠odo 2025-1 completado exitosamente en 38 minutos
```

---

## üìà Mejoras Futuras

### 1. Rate Limiting Inteligente

Ajustar delays autom√°ticamente basado en la tasa de errores:
- Si muchos timeouts ‚Üí aumentar delay entre requests
- Si todo OK ‚Üí reducir delay para ir m√°s r√°pido

### 2. Checkpoint/Resume

Guardar progreso peri√≥dicamente:
- Si el proceso se detiene, reanudar desde la √∫ltima c√©dula procesada
- No rehacer trabajo ya completado

### 3. Procesamiento Distribuido

Dividir per√≠odos en m√∫ltiples jobs paralelos:
- Job 1: 2026-1, 2025-2, 2025-1
- Job 2: 2024-2, 2024-1, 2023-2
- Job 3: 2023-1, 2022-2, 2022-1

---

## ‚úÖ Checklist de Verificaci√≥n

Despu√©s de implementar estos cambios:

- [x] Step timeout aumentado a 180 minutos
- [x] Keep-alive implementado (cada 5 minutos)
- [x] Command timeout de 2 horas por per√≠odo
- [x] REQUEST_TIMEOUT aumentado a 60 segundos
- [x] SHEETS_READ_TIMEOUT aumentado a 120 segundos
- [x] M√°s reintentos configurados (5 en lugar de 3)
- [x] Progreso visible durante delays (cada 5 minutos)
- [x] Job timeout total de 14 horas

---

## üéØ Resultado Esperado

Con estos cambios, el scraper deber√≠a:

‚úÖ **Procesar cada per√≠odo por 2+ horas sin detenerse**
‚úÖ **Mostrar progreso visible cada 5 minutos**
‚úÖ **Manejar timeouts temporales con reintentos**
‚úÖ **Completar todos los 9 per√≠odos en ~8-12 horas**
‚úÖ **No ser interrumpido por idle timeouts**

---

## üìû Soporte

Si el problema persiste despu√©s de estos cambios:

1. **Revisar logs** completos de la ejecuci√≥n
2. **Verificar** mensajes de `[Keep-Alive]` cada 5 minutos
3. **Buscar** mensajes de timeout espec√≠ficos
4. **Contactar** al administrador con los logs

---

## üìÑ Referencias

- [GitHub Actions Timeout](https://docs.github.com/en/actions/learn-github-actions/usage-limits-billing-and-administration#usage-limits)
- [Timeout Command](https://man7.org/linux/man-pages/man1/timeout.1.html)
- [Python Requests Timeout](https://requests.readthedocs.io/en/latest/user/advanced/#timeouts)

