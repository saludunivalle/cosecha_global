"""
Orquestador principal del scraper Univalle
"""

import sys
import logging
import argparse
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from collections import defaultdict
import json
import os

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # Fallback sin barra de progreso
    def tqdm(iterable, desc=None, total=None, **kwargs):
        if desc:
            print(desc)
        return iterable

# Agregar el directorio padre al path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from scraper.config.settings import (
    LOG_LEVEL,
    LOG_FORMAT,
    LOG_FILE,
    validate_config,
    GOOGLE_SHEETS_SPREADSHEET_ID,
    TARGET_PERIOD,
)
from scraper.services.univalle_scraper import UnivalleScraper, DatosDocente
from scraper.services.sheets_service import SheetsService
from scraper.services.period_manager import PeriodManager
from scraper.utils.helpers import (
    validar_cedula,
    limpiar_cedula,
    formatear_nombre_completo,
)


def configurar_logging():
    """Configura el sistema de logging."""
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL.upper()),
        format=LOG_FORMAT,
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )


def procesar_docente(
    scraper: UnivalleScraper,
    sheets_service: SheetsService,
    cedula: str,
    periodos: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Procesa un docente para m√∫ltiples per√≠odos.
    
    Args:
        scraper: Instancia del scraper
        sheets_service: Instancia del servicio de Sheets
        cedula: C√©dula del docente
        periodos: Lista de per√≠odos a procesar
        
    Returns:
        Diccionario con resultados del procesamiento
    """
    logger = logging.getLogger(__name__)
    
    cedula_limpia = limpiar_cedula(cedula)
    
    if not validar_cedula(cedula_limpia):
        raise ValueError(f"C√©dula inv√°lida: {cedula}")
    
    logger.info(f"Procesando docente: {cedula_limpia}")
    
    resultados = {
        'cedula': cedula_limpia,
        'periodos_procesados': [],
        'errores': []
    }
    
    for periodo in periodos:
        periodo_id = periodo['idPeriod']
        periodo_label = periodo['label']
        
        try:
            logger.info(f"Procesando per√≠odo {periodo_label} (ID: {periodo_id})")
            
            # Scraping
            datos = scraper.procesar_docente(cedula_limpia, periodo_id)
            
            # Guardar en Sheets
            guardar_datos_en_sheets(sheets_service, datos, periodo_label)
            
            resultados['periodos_procesados'].append({
                'periodo': periodo_label,
                'idPeriod': periodo_id,
                'pregrado': len(datos.actividades_pregrado),
                'postgrado': len(datos.actividades_postgrado),
                'investigacion': len(datos.actividades_investigacion),
            })
            
            logger.info(
                f"Per√≠odo {periodo_label} completado: "
                f"{len(datos.actividades_pregrado)} pregrado, "
                f"{len(datos.actividades_postgrado)} postgrado, "
                f"{len(datos.actividades_investigacion)} investigaci√≥n"
            )
            
        except Exception as e:
            error_msg = f"Error procesando per√≠odo {periodo_label}: {e}"
            logger.error(error_msg, exc_info=True)
            resultados['errores'].append({
                'periodo': periodo_label,
                'error': str(e)
            })
    
    return resultados


def guardar_datos_en_sheets(
    sheets_service: SheetsService,
    datos: DatosDocente,
    periodo_label: str
):
    """
    Guarda los datos de un docente en Google Sheets.
    
    Args:
        sheets_service: Instancia del servicio de Sheets
        datos: Datos del docente
        periodo_label: Label del per√≠odo
    """
    logger = logging.getLogger(__name__)
    
    info = datos.informacion_personal
    
    # Hoja principal del per√≠odo
    hoja_principal = f"Periodo_{periodo_label}"
    
    # Datos b√°sicos del docente
    fila_principal = [
        info.cedula,
        info.nombre,
        info.apellido1,
        info.apellido2,
        info.unidad_academica or info.escuela,
        info.departamento,
        periodo_label,
        info.vinculacion,
        info.categoria,
        info.dedicacion,
        info.nivel_alcanzado,
        info.cargo,
    ]
    
    sheets_service.agregar_fila(hoja_principal, fila_principal)
    
    # Guardar actividades de pregrado
    if datos.actividades_pregrado:
        hoja_pregrado = f"{hoja_principal}_Pregrado"
        filas_pregrado = []
        
        for actividad in datos.actividades_pregrado:
            fila = [
                info.cedula,
                periodo_label,
                actividad.codigo,
                actividad.nombre_asignatura,
                actividad.grupo,
                actividad.tipo,
                actividad.horas_semestre,
            ]
            filas_pregrado.append(fila)
        
        sheets_service.agregar_filas(hoja_pregrado, filas_pregrado)
        logger.debug(f"Guardadas {len(filas_pregrado)} actividades de pregrado")
    
    # Guardar actividades de postgrado
    if datos.actividades_postgrado:
        hoja_postgrado = f"{hoja_principal}_Postgrado"
        filas_postgrado = []
        
        for actividad in datos.actividades_postgrado:
            fila = [
                info.cedula,
                periodo_label,
                actividad.codigo,
                actividad.nombre_asignatura,
                actividad.grupo,
                actividad.tipo,
                actividad.horas_semestre,
            ]
            filas_postgrado.append(fila)
        
        sheets_service.agregar_filas(hoja_postgrado, filas_postgrado)
        logger.debug(f"Guardadas {len(filas_postgrado)} actividades de postgrado")
    
    # Guardar actividades de investigaci√≥n
    if datos.actividades_investigacion:
        hoja_investigacion = f"{hoja_principal}_Investigacion"
        filas_investigacion = []
        
        for actividad in datos.actividades_investigacion:
            fila = [
                info.cedula,
                periodo_label,
                actividad.codigo,
                actividad.nombre_proyecto,
                actividad.aprobado_por,
                actividad.horas_semestre,
            ]
            filas_investigacion.append(fila)
        
        sheets_service.agregar_filas(hoja_investigacion, filas_investigacion)
        logger.debug(f"Guardadas {len(filas_investigacion)} actividades de investigaci√≥n")


def crear_estructura_hojas(period_manager: PeriodManager, num_periodos: int):
    """
    Crea la estructura de hojas para los per√≠odos.
    
    Args:
        period_manager: Instancia del gestor de per√≠odos
        num_periodos: N√∫mero de per√≠odos a crear
    """
    logger = logging.getLogger(__name__)
    
    logger.info("Creando estructura de hojas...")
    
    periodos = period_manager.obtener_ultimos_n_periodos(num_periodos)
    
    headers = {
        'principal': [
            'C√©dula', 'Nombre', 'Apellido1', 'Apellido2',
            'Escuela', 'Departamento', 'Per√≠odo',
            'Vinculaci√≥n', 'Categor√≠a', 'Dedicaci√≥n',
            'Nivel Alcanzado', 'Cargo'
        ],
        'pregrado': [
            'C√©dula', 'Per√≠odo', 'C√≥digo', 'Nombre Asignatura',
            'Grupo', 'Tipo', 'Horas Semestre'
        ],
        'postgrado': [
            'C√©dula', 'Per√≠odo', 'C√≥digo', 'Nombre Asignatura',
            'Grupo', 'Tipo', 'Horas Semestre'
        ],
        'investigacion': [
            'C√©dula', 'Per√≠odo', 'C√≥digo', 'Nombre Proyecto',
            'Aprobado Por', 'Horas Semestre'
        ],
        'tesis': [
            'C√©dula', 'Per√≠odo', 'C√≥digo Estudiante', 'T√≠tulo Tesis',
            'Plan', 'Horas Semestre'
        ],
        'extension': [
            'C√©dula', 'Per√≠odo', 'Tipo', 'Nombre', 'Horas Semestre'
        ],
        'administrativas': [
            'C√©dula', 'Per√≠odo', 'Cargo', 'Descripci√≥n', 'Horas Semestre'
        ],
        'complementarias': [
            'C√©dula', 'Per√≠odo', 'Tipo', 'Descripci√≥n', 'Horas Semestre'
        ],
        'intelectuales': [
            'C√©dula', 'Per√≠odo', 'T√≠tulo', 'Tipo', 'Descripci√≥n'
        ],
        'comision': [
            'C√©dula', 'Per√≠odo', 'Tipo Comisi√≥n', 'Descripci√≥n'
        ],
    }
    
    period_manager.crear_hojas_periodos(periodos, headers, limpiar_existentes=False)
    
    logger.info(f"Estructura creada para {len(periodos)} per√≠odos")


def enviar_notificacion(errores_criticos: List[str], logger: logging.Logger):
    """
    Env√≠a notificaci√≥n si hay errores cr√≠ticos.
    
    Args:
        errores_criticos: Lista de mensajes de errores cr√≠ticos
        logger: Logger para registrar
    """
    if not errores_criticos:
        return
    
    # Por ahora solo loguear, se puede extender a email/Slack
    logger.error("="*60)
    logger.error("ERRORES CR√çTICOS DETECTADOS:")
    logger.error("="*60)
    for error in errores_criticos:
        logger.error(f"  ‚ùå {error}")
    logger.error("="*60)
    
    # TODO: Implementar notificaciones por email/Slack si se requiere
    # Ejemplo:
    # send_email_notification(errores_criticos)
    # send_slack_notification(errores_criticos)


def agrupar_actividades_por_periodo(actividades: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """
    Agrupa actividades por per√≠odo.
    
    Args:
        actividades: Lista de actividades de un profesor
        
    Returns:
        Diccionario con per√≠odo como clave y lista de actividades como valor
    """
    agrupadas = defaultdict(list)
    
    for actividad in actividades:
        periodo = actividad.get('periodo', '')
        if periodo:
            agrupadas[periodo].append(actividad)
    
    return dict(agrupadas)


def escribir_actividades_en_hojas(
    sheets_service: SheetsService,
    actividades_por_periodo: Dict[str, List[Dict[str, Any]]],
    logger: logging.Logger
):
    """
    Escribe actividades agrupadas por per√≠odo en las hojas correspondientes.
    
    Usa batch write para eficiencia.
    
    Args:
        sheets_service: Servicio de Google Sheets
        actividades_por_periodo: Diccionario con per√≠odo como clave y lista de actividades
        logger: Logger para registrar
    """
    # 14 columnas (sin Detalle actividad, Nivel, Cargo)
    headers = [
        'Cedula',              # 1
        'Nombre Profesor',     # 2
        'Escuela',             # 3
        'Departamento',        # 4
        'Tipo de Actividad',   # 5
        'Categoria',           # 6
        'Nombre de actividad', # 7
        'Numero de horas',     # 8
        'id',                  # 9
        'Per√≠odo',             # 10
        'Porcentaje Horas',    # 11
        'Actividad',           # 12
        'Vinculaci√≥n',         # 13
        'Dedicaci√≥n',          # 14
    ]
    
    for periodo_label, actividades in actividades_por_periodo.items():
        try:
            logger.debug(f"Escribiendo {len(actividades)} actividades para per√≠odo {periodo_label}")
            
            # Convertir diccionarios a listas de valores (14 columnas)
            filas = []
            contador = 0
            for actividad in actividades:
                contador += 1
                # Asegurar que el n√∫mero de horas nunca sea vac√≠o/None y sea float
                horas_semestre = actividad.get('numero_horas', 0.0)
                if horas_semestre in ('', None):
                    horas_semestre = 0.0
                try:
                    horas_semestre = float(horas_semestre)
                except (ValueError, TypeError):
                    logger.warning(f"‚ö†Ô∏è Valor de horas_semestre no convertible a float: {horas_semestre!r}. Usando 0.0")
                    horas_semestre = 0.0
                
                # Extraer porcentaje de horas si existe
                porcentaje_horas = actividad.get('porcentaje', '') or actividad.get('porc', '') or ''
                
                # Extraer ID/c√≥digo de la actividad
                id_actividad = actividad.get('codigo', '') or actividad.get('id', '') or ''
                
                row_data = [
                    actividad.get('cedula', ''),             # 1. Cedula
                    actividad.get('nombre_profesor', ''),    # 2. Nombre Profesor
                    actividad.get('escuela', ''),            # 3. Escuela
                    actividad.get('departamento', ''),       # 4. Departamento
                    actividad.get('tipo_actividad', ''),     # 5. Tipo de Actividad
                    actividad.get('categoria', ''),          # 6. Categoria
                    actividad.get('nombre_actividad', ''),   # 7. Nombre de actividad
                    horas_semestre,                          # 8. Numero de horas (float, nunca vac√≠o)
                    id_actividad,                            # 9. id (c√≥digo de la actividad)
                    actividad.get('periodo', ''),            # 10. Per√≠odo
                    porcentaje_horas,                        # 11. Porcentaje Horas
                    actividad.get('actividad', ''),          # 12. Actividad
                    actividad.get('vinculacion', ''),        # 13. Vinculaci√≥n
                    actividad.get('dedicacion', ''),         # 14. Dedicaci√≥n
                ]
                
                # Validar cantidad de columnas antes de escribir
                if len(row_data) != 14:
                    logger.error(
                        f"‚ùå Row inv√°lido para {actividad.get('cedula', '')}: "
                        f"tiene {len(row_data)} columnas, esperadas 14"
                    )
                    logger.error(f"   Row: {row_data}")
                    continue
                
                # Validar que columna 8 (√≠ndice 7) sea n√∫mero
                if not isinstance(row_data[7], (int, float)):
                    logger.warning(
                        f"‚ö†Ô∏è Horas no es n√∫mero para {actividad.get('cedula', '')}: {row_data[7]!r}"
                    )
                    row_data[7] = 0.0
                
                # Log de ejemplo cada 100 registros
                if contador % 100 == 0:
                    logger.info(f"üìä Ejemplo de row #{contador}:")
                    logger.info(f"   {row_data}")
                
                filas.append(row_data)

            if not filas:
                logger.warning(f"No hay filas v√°lidas para escribir en per√≠odo {periodo_label}")
                continue
            
            # Escribir en batch a la hoja del per√≠odo
            nombre_hoja = periodo_label
            sheets_service.agregar_filas(nombre_hoja, filas)
            
            logger.debug(f"‚úì {len(filas)} filas escritas en hoja '{nombre_hoja}'")
            
        except Exception as e:
            logger.error(f"Error escribiendo actividades para per√≠odo {periodo_label}: {e}", exc_info=True)
            raise


def flujo_completo(
    source_sheet_url: Optional[str] = None,
    source_worksheet: str = "2025-2",
    source_column: str = "D",
    target_sheet_url: Optional[str] = None,
    target_period: Optional[str] = None,
    delay_entre_cedulas: float = 1.0,
    max_cedulas: Optional[int] = None
):
    """
    Flujo completo de scraping para un per√≠odo espec√≠fico:
    
    1. Leer c√©dulas desde Google Sheet
    2. Obtener per√≠odo objetivo (TARGET_PERIOD)
    3. Preparar hoja del per√≠odo
    4. Scrapear cada c√©dula para el per√≠odo
    5. Escribir datos en batch
    6. Logging completo y notificaciones
    
    Args:
        source_sheet_url: URL de la hoja fuente (None = usar hoja por defecto)
        source_worksheet: Nombre de la hoja fuente (default: "2025-2")
        source_column: Columna de c√©dulas (default: "D")
        target_sheet_url: URL de la hoja destino (None = usar hoja por defecto)
        target_period: Per√≠odo a procesar (None = usar TARGET_PERIOD de variable de entorno)
        delay_entre_cedulas: Delay entre c√©dulas en segundos (default: 1.0)
        max_cedulas: M√°ximo n√∫mero de c√©dulas a procesar (None = procesar todas)
    """
    logger = logging.getLogger(__name__)
    inicio_total = time.time()
    
    logger.info("="*80)
    logger.info("INICIANDO FLUJO COMPLETO DE SCRAPING")
    logger.info("="*80)
    logger.info(f"Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    errores_criticos = []
    estadisticas = {
        'cedulas_leidas': 0,
        'cedulas_procesadas': 0,
        'cedulas_con_error': 0,
        'total_actividades': 0,
        'errores_por_cedula': {}
    }
    
    try:
        # 1. Inicializar servicios
        logger.info("\n[PASO 1/5] Inicializando servicios...")
        scraper = UnivalleScraper()
        sheets_service = SheetsService()
        period_manager = PeriodManager(sheets_service, scraper)
        logger.info("‚úì Servicios inicializados")
        
        # 2. Obtener per√≠odo objetivo
        logger.info("\n[PASO 2/5] Obteniendo per√≠odo objetivo...")
        try:
            if not target_period:
                target_period = period_manager.get_target_period()
            else:
                # Validar formato del per√≠odo proporcionado
                from scraper.utils.helpers import parsear_periodo_label
                periodo_info = parsear_periodo_label(target_period)
                if not periodo_info:
                    raise ValueError(
                        f"Formato de per√≠odo inv√°lido: {target_period}. "
                        f"Debe ser en formato 'YYYY-T' (ej: '2026-1', '2025-2')"
                    )
            
            logger.info(f"‚úì Per√≠odo objetivo: {target_period}")
        except Exception as e:
            error_msg = f"Error obteniendo per√≠odo objetivo: {e}"
            logger.error(error_msg, exc_info=True)
            errores_criticos.append(error_msg)
            raise
        
        # 3. Leer c√©dulas desde Google Sheet
        logger.info(f"\n[PASO 3/5] Leyendo c√©dulas desde hoja '{source_worksheet}', columna {source_column}...")
        try:
            # Usar m√©todo batch m√°s eficiente (usa configuraci√≥n de settings)
            if source_sheet_url:
                cedulas = sheets_service.get_cedulas_batch(
                    sheet_url=source_sheet_url,
                    worksheet_name=source_worksheet,
                    column=source_column
                )
            else:
                cedulas = sheets_service.get_cedulas_batch(
                    worksheet_name=source_worksheet,
                    column=source_column
                )
            
            estadisticas['cedulas_leidas'] = len(cedulas)
            logger.info(f"‚úì {len(cedulas)} c√©dulas encontradas")
            
            # Limitar n√∫mero de c√©dulas si se especific√≥ max_cedulas
            if max_cedulas and max_cedulas > 0:
                cedulas_originales = len(cedulas)
                cedulas = cedulas[:max_cedulas]
                logger.warning(
                    f"‚ö†Ô∏è  L√çMITE APLICADO: Procesando {len(cedulas)} de {cedulas_originales} c√©dulas "
                    f"(max_cedulas={max_cedulas})"
                )
            
            if not cedulas:
                raise ValueError(f"No se encontraron c√©dulas en la hoja '{source_worksheet}'")
            
        except Exception as e:
            error_msg = f"Error leyendo c√©dulas: {e}"
            logger.error(error_msg, exc_info=True)
            errores_criticos.append(error_msg)
            raise
        
        # 4. Preparar hoja del per√≠odo
        logger.info(f"\n[PASO 4/5] Preparando hoja para per√≠odo {target_period}...")
        try:
            if target_sheet_url:
                period_manager.prepare_single_period_sheet(
                    sheet_url=target_sheet_url,
                    period=target_period
                )
            else:
                period_manager.prepare_single_period_sheet(
                    period=target_period
                )
            logger.info(f"‚úì Hoja del per√≠odo {target_period} preparada")
        except Exception as e:
            error_msg = f"Error preparando hoja: {e}"
            logger.error(error_msg, exc_info=True)
            errores_criticos.append(error_msg)
            raise
        
        # 5. Obtener ID del per√≠odo objetivo
        logger.info(f"\n[PASO 5/7] Obteniendo ID del per√≠odo {target_period}...")
        try:
            logger.info("Obteniendo per√≠odos disponibles desde el sistema...")
            periodos_disponibles = scraper.obtener_periodos_disponibles()
            logger.info(f"‚úì {len(periodos_disponibles)} per√≠odos disponibles en el sistema")
            
            # Buscar ID del per√≠odo objetivo
            periodo_match = next(
                (p for p in periodos_disponibles if p['label'] == target_period),
                None
            )
            
            if not periodo_match:
                raise ValueError(
                    f"No se encontr√≥ el per√≠odo {target_period} en el sistema. "
                    f"Per√≠odos disponibles: {[p['label'] for p in periodos_disponibles[:10]]}"
                )
            
            periodo_id = periodo_match['idPeriod']
            logger.info(f"‚úì Per√≠odo {target_period} ‚Üí ID: {periodo_id}")
            
        except Exception as e:
            error_msg = f"Error obteniendo ID del per√≠odo: {e}"
            logger.error(error_msg, exc_info=True)
            errores_criticos.append(error_msg)
            raise
        
        # 6. Scrapear cada c√©dula para el per√≠odo objetivo (con soporte de checkpoint)
        logger.info(f"\n[PASO 6/7] Scrapeando {len(cedulas)} c√©dulas para per√≠odo {target_period}...")
        
        # Archivo de checkpoint (por per√≠odo)
        checkpoint_file = f"checkpoint_{target_period.replace('-', '_')}.json"
        
        # Cargar c√©dulas ya procesadas si existe checkpoint
        cedulas_procesadas: List[str] = []
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    cedulas_procesadas = data.get("cedulas_procesadas", [])
                logger.info(f"üîÅ Checkpoint encontrado: {len(cedulas_procesadas)} c√©dulas ya procesadas")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è No se pudo cargar checkpoint '{checkpoint_file}': {e}")
        
        # Filtrar pendientes respetando max_cedulas aplicado antes
        cedulas_pendientes = [c for c in cedulas if limpiar_cedula(c) not in cedulas_procesadas]
        total_cedulas = len(cedulas)
        total_pendientes = len(cedulas_pendientes)
        
        logger.info(
            f"üìä Total c√©dulas a procesar en hoja: {total_cedulas} | "
            f"Ya procesadas (checkpoint): {len(cedulas_procesadas)} | "
            f"Pendientes: {total_pendientes}"
        )
        
        # Acumulador de actividades para el per√≠odo
        actividades_periodo: List[Dict[str, Any]] = []
        errores_cedulas: List[str] = []
        
        if not cedulas_pendientes:
            logger.info("‚úÖ No hay c√©dulas pendientes seg√∫n el checkpoint; se omite scraping.")
        else:
            iterador_cedulas = tqdm(
                cedulas_pendientes,
                desc=f"Scrapeando c√©dulas para {target_period}",
                unit="cedula",
                disable=not HAS_TQDM
            )
            
            for idx, cedula in enumerate(iterador_cedulas, 1):
                if HAS_TQDM:
                    iterador_cedulas.set_description(f"Scrapeando {cedula} - {target_period}")
                
                cedula_limpia = limpiar_cedula(cedula)
                errores_cedula: List[str] = []
                
                logger.info(
                    f"üîÑ Procesando c√©dula {idx} de {total_pendientes} "
                    f"({cedula_limpia}) - global {len(cedulas_procesadas) + 1} de {total_cedulas}"
                )
                
                try:
                    logger.debug(f"Scrapeando {cedula_limpia} para per√≠odo {target_period} (ID: {periodo_id})")
                    actividades_cedula = scraper.scrape_teacher_data(
                        cedula_limpia,
                        id_periodo=periodo_id,
                        max_retries=3,
                        delay_min=0.5,
                        delay_max=1.0
                    )
                    
                    # Asegurar que todas las actividades tengan el per√≠odo correcto
                    for actividad in actividades_cedula:
                        if not actividad.get('periodo') or actividad.get('periodo') != target_period:
                            actividad['periodo'] = target_period
                    
                    if actividades_cedula:
                        actividades_periodo.extend(actividades_cedula)
                        estadisticas['total_actividades'] += len(actividades_cedula)
                        estadisticas['cedulas_procesadas'] += 1
                        logger.info(f"‚úì {cedula_limpia}: {len(actividades_cedula)} actividades extra√≠das")
                    else:
                        # Contar c√©dulas sin actividades separadamente
                        if 'cedulas_sin_actividades' not in estadisticas:
                            estadisticas['cedulas_sin_actividades'] = 0
                        estadisticas['cedulas_sin_actividades'] += 1
                        logger.warning(f"‚ö†Ô∏è {cedula_limpia}: No se encontraron actividades para per√≠odo {target_period}")
                    
                    # Marcar como procesada y guardar checkpoint cada 100
                    cedulas_procesadas.append(cedula_limpia)
                    if len(cedulas_procesadas) % 100 == 0:
                        try:
                            with open(checkpoint_file, "w", encoding="utf-8") as f:
                                json.dump(
                                    {
                                        "cedulas_procesadas": cedulas_procesadas,
                                        "timestamp": datetime.now().isoformat(),
                                        "periodo": target_period,
                                        "total_cedulas": total_cedulas,
                                    },
                                    f,
                                    ensure_ascii=False,
                                    indent=2,
                                )
                            logger.info(f"üíæ Checkpoint guardado: {len(cedulas_procesadas)} c√©dulas procesadas")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è No se pudo guardar checkpoint '{checkpoint_file}': {e}")
                    
                except Exception as e:
                    error_msg = f"Error procesando {cedula_limpia}: {e}"
                    logger.error(error_msg, exc_info=True)
                    errores_cedula.append(str(e))
                    estadisticas['errores_por_cedula'][cedula_limpia] = errores_cedula
                    errores_cedulas.append(cedula_limpia)
                    estadisticas['cedulas_con_error'] += 1
                
                # Delay entre c√©dulas
                if delay_entre_cedulas > 0:
                    time.sleep(delay_entre_cedulas)
        
        # Guardar checkpoint final
        try:
            with open(checkpoint_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "cedulas_procesadas": cedulas_procesadas,
                        "timestamp": datetime.now().isoformat(),
                        "periodo": target_period,
                        "total_cedulas": total_cedulas,
                    },
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
            logger.info(f"‚úÖ Checkpoint final guardado: {len(cedulas_procesadas)} c√©dulas procesadas en total")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è No se pudo guardar checkpoint final '{checkpoint_file}': {e}")
        
        logger.info(f"\n‚úì Scraping completado: {estadisticas['cedulas_procesadas']} exitosas, {estadisticas['cedulas_con_error']} con errores")
        
        # 7. Escribir datos en batch
        logger.info(f"\n[PASO 7/7] Escribiendo datos en hoja del per√≠odo {target_period}...")
        
        if not actividades_periodo:
            logger.warning("‚ö†Ô∏è No hay actividades para escribir")
        else:
            try:
                escribir_actividades_en_hojas(
                    sheets_service,
                    {target_period: actividades_periodo},
                    logger
                )
                logger.info(f"‚úì Per√≠odo {target_period}: {len(actividades_periodo)} actividades escritas")
            except Exception as e:
                error_msg = f"Error escribiendo per√≠odo {target_period}: {e}"
                logger.error(error_msg, exc_info=True)
                errores_criticos.append(error_msg)
        
        # 7. Resumen final
        tiempo_total = time.time() - inicio_total
        logger.info(f"\n[PASO 7/7] Generando resumen final...")
        
        logger.info("="*80)
        logger.info("RESUMEN FINAL")
        logger.info("="*80)
        logger.info(f"Per√≠odo procesado: {target_period}")
        logger.info(f"Tiempo total de ejecuci√≥n: {tiempo_total:.2f} segundos ({tiempo_total/60:.2f} minutos)")
        logger.info(f"C√©dulas le√≠das: {estadisticas['cedulas_leidas']}")
        logger.info(f"C√©dulas procesadas con actividades: {estadisticas['cedulas_procesadas']}")
        logger.info(f"C√©dulas sin actividades para el per√≠odo: {estadisticas.get('cedulas_sin_actividades', 0)}")
        logger.info(f"C√©dulas con errores: {estadisticas['cedulas_con_error']}")
        logger.info(f"Total actividades extra√≠das: {estadisticas['total_actividades']}")
        logger.info(f"Actividades para per√≠odo {target_period}: {len(actividades_periodo)}")
        
        # Errores
        if estadisticas['errores_por_cedula']:
            logger.warning(f"\nErrores por c√©dula ({len(estadisticas['errores_por_cedula'])}):")
            for cedula, errores in list(estadisticas['errores_por_cedula'].items())[:10]:  # Mostrar solo primeros 10
                logger.warning(f"  {cedula}: {errores[0] if errores else 'Error desconocido'}")
            if len(estadisticas['errores_por_cedula']) > 10:
                logger.warning(f"  ... y {len(estadisticas['errores_por_cedula']) - 10} m√°s")
        
        # Notificaciones si hay errores cr√≠ticos
        if errores_criticos or estadisticas['cedulas_con_error'] > 0:
            enviar_notificacion(errores_criticos, logger)
        
        logger.info("="*80)
        logger.info(f"Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*80)
        
        return {
            'exito': True,
            'estadisticas': estadisticas,
            'errores_criticos': errores_criticos,
            'tiempo_total': tiempo_total
        }
        
    except Exception as e:
        tiempo_total = time.time() - inicio_total
        error_msg = f"Error fatal en flujo completo: {e}"
        logger.error(error_msg, exc_info=True)
        errores_criticos.append(error_msg)
        enviar_notificacion(errores_criticos, logger)
        
        return {
            'exito': False,
            'estadisticas': estadisticas,
            'errores_criticos': errores_criticos,
            'tiempo_total': tiempo_total,
            'error': str(e)
        }


def main():
    """Funci√≥n principal del orquestador."""
    configurar_logging()
    logger = logging.getLogger(__name__)
    
    parser = argparse.ArgumentParser(
        description='Scraper de datos acad√©micos del portal Univalle - Flujo completo'
    )
    
    parser.add_argument(
        '--modo',
        type=str,
        choices=['completo', 'individual', 'archivo'],
        default='completo',
        help='Modo de ejecuci√≥n: completo (default), individual, archivo'
    )
    
    # Argumentos para flujo completo
    parser.add_argument(
        '--source-sheet-url',
        type=str,
        help='URL de la hoja fuente con c√©dulas (opcional, usa hoja por defecto si no se especifica)'
    )
    
    parser.add_argument(
        '--source-worksheet',
        type=str,
        default='2025-2',
        help='Nombre de la hoja fuente (default: "2025-2")'
    )
    
    parser.add_argument(
        '--source-column',
        type=str,
        default='D',
        help='Columna de c√©dulas en hoja fuente (default: "D")'
    )
    
    parser.add_argument(
        '--target-sheet-url',
        type=str,
        help='URL de la hoja destino para escribir datos (opcional, usa hoja por defecto si no se especifica)'
    )
    
    parser.add_argument(
        '--target-period',
        type=str,
        default=None,
        help='Per√≠odo objetivo a procesar (ej: "2026-1"). Si no se especifica, usa TARGET_PERIOD de variable de entorno'
    )
    
    parser.add_argument(
        '--delay-cedulas',
        type=float,
        default=1.0,
        help='Delay entre c√©dulas en segundos (default: 1.0)'
    )
    
    parser.add_argument(
        '--max-cedulas',
        type=int,
        default=None,
        help='M√°ximo n√∫mero de c√©dulas a procesar (default: None, procesa todas)'
    )
    
    # Argumentos para modo individual
    parser.add_argument(
        '--cedula',
        type=str,
        help='C√©dula del docente a procesar (modo individual)'
    )
    
    parser.add_argument(
        '--periodos',
        type=int,
        default=8,
        help='N√∫mero de per√≠odos a procesar (default: 8)'
    )
    
    parser.add_argument(
        '--cedulas-archivo',
        type=str,
        help='Ruta a archivo con lista de c√©dulas (modo archivo)'
    )
    
    parser.add_argument(
        '--crear-hojas',
        action='store_true',
        help='Crear estructura de hojas antes de procesar'
    )
    
    parser.add_argument(
        '--limpiar-hojas',
        action='store_true',
        help='Limpiar hojas existentes antes de procesar'
    )
    
    args = parser.parse_args()
    
    try:
        # Validar configuraci√≥n
        validate_config()
        logger.info("‚úì Configuraci√≥n validada correctamente")
        
        # Inicializar servicios
        scraper = UnivalleScraper()
        sheets_service = SheetsService()
        period_manager = PeriodManager(sheets_service, scraper)
        
        # Ejecutar seg√∫n modo
        if args.modo == 'completo':
            logger.info("Ejecutando flujo completo...")
            resultado = flujo_completo(
                source_sheet_url=args.source_sheet_url,
                source_worksheet=args.source_worksheet,
                source_column=args.source_column,
                target_sheet_url=args.target_sheet_url,
                target_period=args.target_period,
                delay_entre_cedulas=args.delay_cedulas,
                max_cedulas=args.max_cedulas
            )
            
            if not resultado['exito']:
                sys.exit(1)
        
        elif args.modo == 'individual':
            # Modo individual (mantener compatibilidad)
            if not args.cedula:
                parser.error("--cedula es requerido en modo individual")
            
            periodos = period_manager.obtener_ultimos_n_periodos(args.periodos)
            resultado = procesar_docente(scraper, sheets_service, args.cedula, periodos)
            
            logger.info("Procesamiento completado:")
            logger.info(f"  Per√≠odos procesados: {len(resultado['periodos_procesados'])}")
            logger.info(f"  Errores: {len(resultado['errores'])}")
        
        elif args.modo == 'archivo':
            # Modo archivo (mantener compatibilidad)
            if not args.cedulas_archivo:
                parser.error("--cedulas-archivo es requerido en modo archivo")
            
            with open(args.cedulas_archivo, 'r', encoding='utf-8') as f:
                cedulas = [line.strip() for line in f if line.strip()]
            
            periodos = period_manager.obtener_ultimos_n_periodos(args.periodos)
            
            resultados_totales = {
                'exitosos': 0,
                'errores': 0,
                'detalles': []
            }
            
            for cedula in tqdm(cedulas, desc="Procesando c√©dulas", disable=not HAS_TQDM):
                try:
                    resultado = procesar_docente(scraper, sheets_service, cedula, periodos)
                    resultados_totales['exitosos'] += 1
                    resultados_totales['detalles'].append(resultado)
                except Exception as e:
                    logger.error(f"Error procesando {cedula}: {e}", exc_info=True)
                    resultados_totales['errores'] += 1
            
            logger.info("Procesamiento masivo completado:")
            logger.info(f"  Exitosos: {resultados_totales['exitosos']}")
            logger.info(f"  Errores: {resultados_totales['errores']}")
        
        logger.info("‚úì Proceso completado exitosamente")
        
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è Proceso interrumpido por el usuario")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Error fatal: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()

